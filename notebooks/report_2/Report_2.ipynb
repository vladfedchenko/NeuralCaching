{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Following your advice, during the debug process I will focus on the case with single population of 10_000 items. If the results of training will be good in this case then I will expand to the case with mixed population."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first things I've decided to explore is the **learning curves** when **sigmoid** activation is applied on all layers. The dataset has **100_000 rows**, train-validation split is **0.8**. \n",
    "\n",
    "* The **input** is fraction of requests in previous 4 time slots transformed by $ log(10^{-5} + frac) $, the **output** is unchanged.\n",
    "* learning rate: 0.1\n",
    "* number of iterations: 1000\n",
    "* stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1_err_curves.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like the model did not learned much after the first iteration and the validation error is 2 orders of magnitude higher than the optimal. Let's see what predictions this model makes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<img src=\"2_order_plot.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model learnt that the popularity is exaclty the same (except item #1, for which the popularity is a small bit lower).<br\\>\n",
    "Let's see how the optimal predictor, average of previous time slots, behaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3_order_plot.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's confirm that the torch implementation behaves the same, only this time I will limit the number of iterations to 200, since it seems that in previous case the learning finished much earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Learning curves | Ordering |\n",
    "| --- | --- |\n",
    "|<img src=\"4_err_curves.png\"/>|<img src=\"4_order_plot.png\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The final error is comparable in both cases: $1.5 * 10^{-7}$ and $2.3 * 10^{-7}$. The predicted popularities are also comparable, but in torch case there is a small deviation from the flat line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I decided to repeat the same experiment for neural networks with linear activations. Since with such high learning rate the learning was divirging, I reduced the learning rate to 0.001 and the number of iterations back to 1000.<br/>\n",
    "After the first batch the error was much larger than after all the following, so I removed it from the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Learning curves | Ordering |\n",
    "| --- | --- |\n",
    "| My NN <img src=\"5_err_curves.png\"/> | My NN <img src=\"5_order_plot.png\"/> |\n",
    "| Torch <img src=\"6_err_curves.png\"/> | Torch <img src=\"6_order_plot.png\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted popularities follow the real popularities closer in this case, but for some items the popularity is predicted to be negative. The error is the same as in the case of sigmoid activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
