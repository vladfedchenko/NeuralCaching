{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To investigate why NN predictions are lower than calculated theoretical values I've decided to eplore what predictions mean predictor does. I've fixed the seed when selecting items to do predictions, so the predictions are made on the same items. Here are NN predictions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| No label | With label |\n",
    "| --- | --- |\n",
    "| <img src=\"1/order_plot_nn_no_label.png\"/> | <img src=\"1/order_plot_nn_with_label.png\"/> |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And the mean predictor:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"1/order_plot_mean.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predictions made by mean predictor are also seem to be biased to be lower than calculated theoretical values. Let's check that the problem is not in generated trace. In the trace the ratio of items from first and second population should be 1:1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Number of requests in trace:\n",
    "<img src=\"2/all.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First population: 4997831 <br/>\n",
    "Second population: 5002169 <br/>\n",
    "There are even slightly more requests from second population, so the problem is not in trace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While closely inspecting predictions made by mean predictor on second population I've observed that the mean of predictions is close to caclulated theoretical value: $ 9.99487524808 * 10^{-5} $. <br/>\n",
    "But the median is $5.85074972881 * 10^{-5}$. <br/>\n",
    "Low median of the predictions and preference to low popularity items after $ -log(frac + 10^{-5}) $ transformation is the possible explanation of behaviour of the NNs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, mean and median of predictions made by NN.<br/> \n",
    "Case with label: <br/>\n",
    "Mean: $ 5.41192173486 * 10^{-5} $ <br/>\n",
    "Median: $ 5.4169656957 * 10^{-5} $ <br/>\n",
    "Case without label: <br/>\n",
    "Mean: $ 4.81817910687 * 10^{-5} $ <br/>\n",
    "Median: $ 4.69565798293 * 10^{-5} $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's address the problem of validation error being lower than training error. <br/>\n",
    "First of all, I've hound a seed for the random generator using which the validation error is smaller than training error after first few iterations. Let's confirm that it doesn't become larger eventually."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"3_err_plot.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The validation error stays lower even after a large number of iterations. But in case of using average predictor (which is the best linear predictor) the error on validation set is also smaller, and the difference between these two errors is close to the one present in NN error evaluation. From this it is possible to conclude that the initial training/validation split influences how the train and validation set errors will compare. Let's establish an experiment - to compare how the difference between training and validation errors compares when using NN and average predictor and different train/validation split."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"4_split_error_plot.png\" style=\"width: 700px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing 100 different splits the trend is apparent - the difference is always close when using NN with Leaky ReLU activation and average predictor. <br/>\n",
    "Maybe Leaky ReLU activation does not introduce enough nonlinearity to allow NN overfit the training set and produce lower error on the training set always. Let's try to use SoftPlus, which is $ ln(1 + e^x) $, as activation.\n",
    "<img src=\"5_softplus.png\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Error curves|Predictions|\n",
    "| --- | --- |\n",
    "|<img src=\"6/err_plot.png\"/>|<img src=\"6/order_plot.png\"/>|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SoftMax activation didn't solve the problem and even showed higher error than ReLU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following Giovanni's advice, it is possible that the used constant $ 10^{-5} $ is too high. Let's chech what results will $10^{-8}$ show."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Error curves|Predictions|\n",
    "| --- | --- |\n",
    "|No label <img src=\"7/1_err_plot.png\"/>|No label<img src=\"7/1_order_plot.png\"/>|\n",
    "|With label <img src=\"7/2_err_plot.png\"/>|With label<img src=\"7/2_order_plot.png\"/>|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
