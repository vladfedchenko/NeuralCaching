
\section{Abstract}
Since the inception of computer science, data retrieval speed was a major bottleneck for numerous applications. The problem got only worse following the introduction of the Internet. To overcome this limitation the concept of caching was introduced. The solution is to store the data which is used the most closer to the location where it is used or store it in a storage with higher access speed. A large number of caching policies have been proposed but most of them require ad-hoc tuning of different parameters and none emerges as a clear winner across different request traces. For this reason, most of the practical caching systems adopt LRU (Least Recently Used) policy because of its simplicity and relatively good performance.

During the internship, we explore the possibility of the application of machine learning algorithms to solve the caching problem. We propose a caching policy which utilizes feedforward neural network and overperforms state of the art policies on both synthetic and real-world request traces. We also examine other attempts of application of machine learning techniques to handle the problem and compare their performance with the approach proposed by us.
