\section{Related work} \label{related_work}

During last few years, a number of related articles appeared. In the next section, we will give a quick review of them and justify the uniqueness of our proposed approach.

The first reviewed article is "Competitive caching with machine learned advice" by Thodoris Lykouris and Sergei Vassilvitskii \cite{18}. This article is mostly a theoretical overview of the ability to apply machine learning algorithms for online caching scenarios. In their study, the authors assume the machine learning component to be a complete black box with unknown inner workings and exact distribution of errors. Then the authors expand by providing an algorithm to aid in the classical caching problem with the application of such a machine learning oracle. They prove that as the error of the oracle decreases the performance of such an algorithm increases and the performance is always capped by a lower bound which can be achieved even without oracle's predictions. The authors confirm their calculations by obtaining some experimental results. The results of the work done by the authors suggest that it is possible to construct a caching policy based on predictions made by a machine learning algorithm and achieve good performance.

Further examination of the topic revealed some attempts of application of learning algorithms trying to improve the performance in multi-node cooperative caching networks \cite{19}, explore the advantages, drawbacks and scalability possibilities of such an approach. While this caching method is also a perspective field, we are going to stick to a classical setting with a single caching node.

While the classical approach is to reactively decide if to cache the object when it is requested, the alternative is to proactively fetch the object if there are reasons to assume that the object is going to be requested in the future. Another batch of articles \cite{20, 21} try to propose a solution to handle the caching problem by using this approach. The authors of \cite{20} are trying to estimate the gains of proactive fetching in the context of 5G cellular network base stations, which in part overlaps with multi-node caching since every wireless base station can be considered as a caching node of a larger network. The authors of \cite{21} propose a particular approach for proactive caching which relies on reinforcement learning technique. Reinforcement learning is known to produce unstable results so we will avoid it during our research. Moreover, our approach is dealing with classical reactive caching so it can be considered original in relation to this articles.

The final batch of the articles is the closest by nature to our approach. The first of the reviewed articles from this batch is "A Deep Reinforcement Learning-Based Framework for Content Caching" \cite{22}. The authors of the article also utilize deep reinforcement learning framework which, as said before, not always produces stable results. Also, the authors do not test their approach on real-world data. The tests on the synthetic data are also not convincing since the data is generated with a small number of unique objects and a small number of requests. The authors in \cite{23} apply a different model for predictions - recurrent neural networks, deep long short-term memory network in particular. In both \cite{22} and \cite{23}, the authors do not justify why they are using complex prediction models while bypassing more simple model as in our approach. Another issue with the approach proposed in \cite{23} is the usage of one-hot encoding in the cache eviction decision process which does not scale well with a large number of the unique object usually encountered in caching. Anyway, further in the report we will reproduce the approach proposed in \cite{23} and compare it with our approach. 

Article \cite{24} is a continuation of work done in \cite{23} with an attempt to extend the application of the policy to multi-node cooperative caching. This may also be a good continuation of development of our approach but it is not explored in the report.


The most similar approach in comparison to our we found in "Popularity-Driven Content Caching" by Suoheng Li, Jie Xu, Mihaela van der Schaar and Weiping Li \cite{25}. The similarities include:
\begin{itemize}
	\item Caching decisions are based on the prediction of the popularity of the objects estimated by some criteria.
	\item Maintenance of priority queue to decide which items to remove from the cache. The priority key is the predicted popularity.
\end{itemize}
Nevertheless, there are some key differences. First of all, the authors of \cite{25} apply a technique of "Adaptive Context Space Partitioning" to determine the popularity of the content while we apply a neural network for this task. This approach implies the mapping of each request with its metadata to a point in k-dimensional space. When a hypercube accumulates a large number of requests it is split. The popularity of the object is determined by its hypercube, or by two variables - the number of received requests in the hypercube and the sum of the revealed future request rate for those requests, both of which are maintained for each hypercube. Second of all, the mechanism of the update of the priority queue is different. Our approach is based on the update of the priority of random individual objects in the queue at every cache hit and the approach proposed in \cite{25} updates the whole queue after each K requests.

Overall, no substantial work has been done in the application of machine learning techniques for caching. The reviewed approaches claim to overperform established policies, but, as stated previously, have some disadvantages or have not been thoroughly validated. Our proposed policy will bring some light to the research on the topic.