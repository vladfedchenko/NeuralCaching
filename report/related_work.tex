\section{Related work} \label{related_work}

During the last few years, a number of articles describing the usage of machine learning algorithms for caching purposes appeared. In this chapter, we will give a quick review of them and justify the uniqueness of our proposed approach.

The first reviewed article is \cite{18}. This article has mostly a theoretical flavour describing how a machine learning oracle can be used to design online caching algorithms with strong worst case guarantees. In their study, the authors assume the machine learning component to be a complete black box with unknown inner workings and exact distribution of generalization errors. Then the authors expand by providing a modification of Marker algorithm \cite{26} with the application of a machine learning oracle. They prove that as the error of the oracle decreases the performance, in terms of competitive ratios, of the proposed algorithm increases and the performance is always capped by a lower bound which can be achieved even without oracle's predictions. The authors complement their findings with an empirical evaluation of the proposed algorithm on real-world data. The results of the work done by the authors suggest that it is possible to construct a caching policy based on predictions made by a machine learning algorithm and achieve good performance.

In most scenarios caching is performed reactively, i.e. the algorithm decides if caching a given object when this is requested. The alternative is to proactively fetch the object if there are reasons to assume that the object is going to be requested in the future. Papers \cite{20, 21} propose a solution to handle the caching problem by using this approach. The authors of \cite{20} are trying to estimate the gains of proactive fetching in the context of 5G cellular network base stations, which in part overlaps with multi-node caching since every wireless base station can be considered as a node of a larger caching network. The authors of \cite{21} propose a particular approach for proactive caching which relies on reinforcement learning technique. Reinforcement learning is known to produce unstable results so we will avoid it during our research. Moreover, our approach is dealing with classical reactive caching so it can be considered orthogonal to that of these articles.

The final batch of articles is the closest by nature to our approach. The authors of \cite{22} also utilize deep reinforcement learning framework which, as said before, not always produces stable results. Also, the authors do not test their approach on real-world data. The tests on synthetic data are also not convincing since the data is generated with a small number of unique objects and a small number of requests leaving some doubts about the scalability of their approach. The authors in \cite{23} apply a different model for predictions - recurrent neural networks, deep long short-term memory network in particular. In both \cite{22} and \cite{23}, the authors do not justify why they are using such complex machine learning techniques while bypassing the more simple models considered in this report. Another issue with the approach proposed in \cite{23} is the usage of one-hot encoding in the cache eviction decision process which does not scale well with the large number of the unique object usually encountered in caching. We have implemented the approach proposed in \cite{23} and compared it with our approach. The results are discussed in Section \ref{comparison}. 

Paper \cite{25} also shows some similarities with our work, even if it does not utilize a machine learning algorithm. The similarities include:
\begin{itemize}
	\item Caching decisions are based on the prediction of the popularity of the objects estimated by some criteria.
	\item A priority queue is maintained to decide which items to remove from the cache. The priority key is the predicted popularity.
\end{itemize}
Nevertheless, there are some key differences. First of all, to determine the popularity of the content the authors of \cite{25} apply a technique named ``Adaptive Context Space Partitioning,'' which they describe in Section V.C.,  while we apply a neural network for this task. This approach implies the mapping of each request with its metadata to a point in a k-dimensional space. When a hypercube accumulates a large number of requests it is split. The popularity of the object is determined by its hypercube, or by two variables - the number of received requests in the hypercube and the sum of the revealed future request rate for those requests, both of which are maintained for each hypercube. Revealed future request rate variable is regularly updated when the true popularity of the content is revealed. Second of all, the mechanism of the update of the priority queue is different. Our approach is based on the update of the priority of random individual objects in the queue at every cache hit and the approach proposed in \cite{25} updates the whole queue every K requests.

Further examination of the topic revealed some attempts of application of learning algorithms trying to improve the performance in multi-node cooperative caching networks \cite{19}, explore the advantages, drawbacks and scalability possibilities of such an approach. While this caching method is also a perspective field, we are going to stick to a classical setting with a single caching node. Article \cite{24} is a continuation of work done in \cite{23} with an attempt to extend the application of the policy to multi-node cooperative caching. This may also be a promising direction for future research but it is not explored in the report.

Overall, no substantial work has been done in the application of machine learning techniques for caching. The reviewed approaches claim to overperform established policies, but, as stated previously, have some disadvantages or have not been thoroughly validated. Our proposed policy will bring some light to the research on the topic.