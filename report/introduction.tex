\section{Introduction} \label{introduction}
\subsection{Caching problem} \label{caching_problem}

The invention of the computer allowed scientists to process vast amounts of data faster than ever before. However, soon a significant bottleneck was discovered - data retrieval speed. The introduction of the Internet only increased the relevance of this problem. According to Cisco, annual global IP traffic is predicted to reach 3.3 zettabytes by 2021 \cite{1}. A massive increase in traffic volume naturally increases the load on the infrastructure. In order to improve performance in various applications and to reduce the impact of traffic growth the concept of caching was introduced. The idea behind this concept is to put the actively used data into storage from which it can be retrieved quicker. The goal is to reduce latency, shorten data access times and improve input/output. Since the workload of most of the applications is highly dependent upon I/O operations, caching positively influences applications performance. Previously described goals can be achieved by using a storage device which is physically closer to the data consumer or which has a higher data access speed. To maximize the utility of the storage devices various caching policies have been introduced. It is impossible to store every object in the cache since the storage capacity is limited. Ideally, we would like to guarantee that the next requested object is stored in the cache. Caching algorithms try to predict the next requests in a variety of ways.

\subsection{Caching policies} \label{caching_policies}

To compare the performance of caching policies we are going to use the cache hit ratio metric which is the most commonly used and effective metric for cache performance evaluation \cite{5}. The cache hit ratio is the ratio of the number of cache hits to the number of requests.

Belady's MIN algorithm is proven to maximize the hit ratio when the size of all objects is the same \cite{2}. The general idea behind this algorithm is to evict from the cache objects which are requested furthest in the future compared to other objects in the cache. However, this information is not available in most settings thus this algorithm cannot be deployed in a practical system.

First In First Out (FIFO) is one of the first proposed caching policies. It is simple to implement and deploy, but eventually has been replaced by more sophisticated algorithms with better performance in terms of hit rate. The unbeaten advantage of FIFO is a low computational complexity of $ O(1) $.

Least Recently Used (LRU) is the natural evolution of FIFO and the most commonly used caching replacement policy. It offers comparably good performance and does not require a lot of extra storage or CPU time. The policy requires to maintain a priority queue what leads, in general, to a time complexity of $ O(\log\,C) $ where $C$ is the size of the cache.

Least Frequently Used (LFU) in some cases overperforms LRU, in particular it is optimal in the long-term if the objects have static popularity. But requires to track the number of requests for all of the objects observed. More importantly, its performance deteriorates rapidly when popularities change over time.

Adaptive Replacement Cache (ARC) \cite{3} is a caching policy introduced by IBM in 2004. It offers better performance than LRU while keeping low computational resources requirements. Roughly, the algorithm requires maintenance of 2 priority queues with some additional constant time operations. Thus, the time complexity is also $ O(\log\,C) $ but the constant is larger than for LRU. It is considered to be state of the art.

While a large number of caching policies has been introduced, there is still room for improvement in comparison to the optimal algorithm. Moreover, since, as said before, the amount of web traffic is expected to rise, even a small improvement in caching policy performance could lead to significant cost savings in long-term.

\subsection{Neural networks} \label{neural_networks_intro}

Following recent successful attempts of application of neural networks \cite{6} for complex task solving \cite{7,8,9} a question arises: is it possible to apply Neural Networks to learn online a close-to-optimal caching policy? To tackle this problem, we will try to apply a simple feedforward fully connected neural network with the goal to construct a new caching policy which overperforms existing methods. In particular, our caching policy will apply a neural network to predict the future popularity of objects.

\subsection{Report organization} \label{report_organization}

In Chapter \ref{related_work}, we will discuss related work in the area.

In Chapter \ref{datasets}, we will continue discussing what data is required to develop and test the proposed caching policy. For ease of development, a controlled and customizable environment is required. Thus we will discuss techniques to generate synthetic data which may be representative at real world scenarios. We will continue by discussing also some real-world data used to test the performance of the proposed policy.

After that, in Chapter \ref{neural_network}, we will discuss in more detail the concept of neural networks, how we use neural networks for caching and the iterative process of tuning the architecture of the network.

In the last part, Chapter \ref{caching_policy}, we will propose an architecture of a caching policy which exploits a neural network to make caching decisions. We will compare the performance of the proposed policy with other approaches recently proposed in the literature.

A subset of the results in this report is going to be submitted to workshop on AI in networks (WAIN) colocated with IFIP WG Performance 2018.



