\section{Introduction} \label{introduction}
\subsection{Caching problem} \label{caching_problem}

The invention of the computer allowed scientists to process vast amounts of data faster than ever before. However, soon a significant bottleneck was discovered - data retrieval speed. The introduction of the Internet only increased the relevance of this problem. According to Cisco, annual global IP traffic is predicted to reach 3.3 zettabytes by 2021 \cite{1}. A massive increase in traffic volume naturally increases the load on the infrastructure. In order to improve performance in various applications and to reduce the impact of traffic growth the concept of caching was introduced. The idea behind this concept is to put the actively used data into storage from which it can be retrieved quicker. The goal is to reduce latency, shorten data access times and improve input/output. Since the workload of most of the applications is highly dependent upon I/O operations, caching positively influences applications performance. Previously described goals can be achieved by using a storage device which is physically closer to the data consumer or which has a higher data access speed. To maximize the utility of the storage devices various caching policies have been introduced. It is impossible to store every object in the cache since the storage capacity is limited. Ideally, we would like to guarantee that the next requested object is stored in the cache. Caching algorithms try to predict this in a variety of ways.

\subsection{Caching policies} \label{caching_policies}

To compare the performance of caching policies we are going to use cache hit ratio \cite{5} metric which is the most commonly used and effective metric for cache performance evaluation.

Belady's min algorithm is proven to minimize the number of cache misses which is the optimal behavior when the size of all objects is the same \cite{2}. The general idea behind this algorithm is to evict from the cache objects which are requested furthest in the future compared to other objects in the cache. However, this information is not available in most settings thus this algorithm cannot be deployed in a practical system.

First In First Out (FIFO) is one of the first proposed caching policies. Simple to implement and deploy but eventually has been replaced by more sophisticated algorithms with better performance in terms of hit rate. The unbeaten advantage of FIFO is a low computational complexity of $ O(1) $.

Least Recently Used (LRU) is the natural evolution of FIFO and the most commonly used caching replacement policy. It offers comparably good performance and does not require a lot of extra storage or CPU time. The policy requires maintenance of a priority queue what leads, in general, to a time complexity of $ O(log\,C) $ where $C$ is the size of the cache.

Least Frequently Used (LFU) in some cases overperforms LRU, it is optimal in long-term if the objects have static popularity in particular. But requires to track the number of requests for all of the objects observed. This disadvantage limits the number of applications of LFU.

Adaptive Replacement Cache (ARC) \cite{3} is a caching policy introduced by IBM \cite{4} in 2004. It offers better performance than LRU while keeping low computational resources requirements. Roughly, the algorithm requires maintenance of 2 priority queues with some additional constant time operations. Thus, the time complexity is also $ O(log\,C) $ but the constant is larger. It is considered to be state of the art.

While a large number of caching policies has been introduced, there is still room for improvement in comparison to the optimal algorithm. Moreover, since, as said before, the amount of web traffic is expected to rise, even a small improvement in caching policy performance could lead to significant cost savings in long-term.

\subsection{Neural networks} \label{neural_networks_intro}

Following recent successful attempts of application of neural networks \cite{6} for complex task solving \cite{7,8,9} a question arises - is it possible to apply Neural Networks to learn online a close to optimal caching policy? To tackle this problem, we will try to apply simple feedforward fully connected neural network with a goal to construct a new caching policy which would overperform existing methods. The primary challenge is to construct a policy which would utilize a neural network efficiently. For example, we will try to apply a neural network to predict the future popularity of objects.

\subsection{Report organization} \label{report_organization}

In the Chapter \ref{related_work}, we will discuss related work in the area.

In the Chapter \ref{datasets}, we continue by discussing what data is required to develop and test the proposed caching policy. For ease of development, a controlled and customizable environment is required. Thus we will discuss techniques to generate synthetic data which is good at representing the real world. We will continue by discussing what real-world data is used to test the performance of the proposed policy.

After that, in the Chapter \ref{neural_network}, we will discuss in more detail the concept of neural networks, how we use neural networks for caching and the iterative process of tuning the architecture of the network.

In the last part, Chapter \ref{caching_policy}, we will propose an architecture of a caching policy which exploits a neural network to make caching decisions. We will compare the performance of the proposed policy with other approaches including the state of the art approaches.



